---
title: "Series chronologiques (vidéo)"
---

```{r}
#| label: setup
#| include: false
#| echo: false
library(forecast)
library(plotrix)
library(viridisLite)
library(ggplot2)
library(survminer)
library(treemap)
library(psy)
library(qgraph)
library(ape)
library(survival)
library(httpgd)
library(psy)
knitr::opts_chunk$set(echo = TRUE)

load("~/Documents/Projets/M2biostatistiques/Cours/CUSM_data/CUSM")
gs <- read.csv2("~/Documents/Projets/M2biostatistiques/Cours/CUSM_data/GoogleSuicide20172022.csv")
load("~/Documents/Projets/M2biostatistiques/Cours/CUSM_data/dataAQRlivre")
data(expsy)
alzh = read.csv("~/Documents/Projets/M2biostatistiques/Cours/alzheimer.csv")
```

## Introduction aux séries chronologiques
Méthodes SARIMAX / ARMA / ARIMA

-   SARIMAX : Seasonal Autoregressive Integrated Moving Average with eXogenous regressors

-   ARMA : Autoregressive Moving Average

-   ARIMA : Autoregressive Integrated Moving Average

### Modèle auto-régressif d'ordre 1 :
Soit une variable $Y$ mesurée au temps $i$

-   Régression de Y sur ce qui se passe au temps $i-1$ : $Y_(i) = (a) + bY_(i-1) + \epsilon$

    -   Ce qui se passe au temps $i$ = une fonction linéaire de ce qui se passe au temps $i-1$ + un bruit aléatoire

    -   Processus auto-régressif d'ordre 1 (AR(1))

On observe une variable Y dans le temps : $Y_1$, $Y_2$, $Y_3$.

Le modèle AR(1) écrit :

$Y_i = a + bY_{i-1} + \varepsilon_i$

où $\varepsilon_i$ est un bruit aléatoire indépendant d’un instant à l’autre, c'est à dire qu'il n'y a pas de corrélation entre les $\varepsilon_i$ entre eux

#### Qu’est-ce qu’une régression (dans ce contexte) ?
Une régression est un modèle qui cherche à expliquer une variable par une autre, en supposant une relation linéaire.

Dans un AR(1), la variable explicative n’est pas une autre variable, mais l**a même variable observée juste avant**, c’est-à-dire $Y_{i-1}$.

C’est donc une régression temporelle, ou auto-régression :

-   $Y_{i-1}$ est l’information disponible au temps précédent,

-   $Y_i$ est ce qu’on cherche à prédire,

-   $\varepsilon_i$ représente la part de $Y_i$ qui n’est pas expliquée par $Y_{i-1}$.

Autrement dit : le présent dépend linéairement du passé (*= c'est une fonction linéaire*), plus un terme aléatoire.

#### Pourquoi dit-on souvent que « $a = 0$ » ?
Le coefficient $a$ est une constante, comme l’intercept dans une régression classique.

Cependant, dans la majorité des cours et logiciels :

1.  La série est centrée avant modélisation.

    -   Centrer consiste à soustraire la moyenne : $Y_i \leftarrow Y_i - \bar Y$.

    -   Une série centrée a une moyenne égale à 0.

2.  Si la série est centrée, alors le terme constant devient inutile.

    -   En effet, **la constante sert justement à ajuster une moyenne non nulle**.

    -   Donc, dans une série centrée : $Y_i = bY_{i-1} + \varepsilon_i$

    -   et on peut prendre $a = 0$ par convention.

#### Variance : définition générale
La variance est une mesure de la dispersion d’une variable autour de sa moyenne.

-   Une grande variance = la variable varie beaucoup.

-   Une petite variance = la variable varie peu.

Mathématiquement :

$\text{Var}(Y) = E[(Y - E[Y])^2]$

Dans un AR(1), il y a deux sources de variance :

1.  la variance du bruit $\varepsilon_i$

2.  la propagation de ce bruit à travers le coefficient $b$

#### Variance du bruit : ce que c’est exactement
Le bruit $\varepsilon_i$ est un terme aléatoire supposé :

-   d’espérance 0 (= moyenne, c'està dire que le bruit n'a pas de tendance moyenne à être positif ou négatif),

-   indépendant d’un instant à l’autre,

-   de variance constante $\sigma^2$.

Cette variance $\sigma^2$ mesure l’amplitude des fluctuations imprévisibles.

C’est la “source” de variabilité du modèle.

Si $\sigma^2$ est grande : chaque instant reçoit un choc important.

Si $\sigma^2$ est petite : les variations imprévisibles sont faibles.

::: callout-note
**"Choc" lié à une variance importante :**

**Le Rôle de l'Erreur (Le Choc Ponctuel)**

Dans le modèle AR(1) ($_i = a + bY_{i-1} + \varepsilon_i$), l'erreur $\varepsilon_i$ est appelée le **bruit aléatoire**.

-   Le bruit $\varepsilon_i$ représente la part de la valeur actuelle $Y_i​$ qui **n'est pas expliquée** par la valeur précédente $Y_{i-1}$.

-   Ce bruit est la **source de variabilité** du modèle. Sa variance ($\sigma^2$) mesure l'amplitude des fluctuations imprévisibles à un instant donné.

-   Dans une **régression ordinaire (non temporelle)**, cette erreur $\varepsilon_i$ affecterait uniquement la valeur courante $Y_i$, puis son effet s'arrêterait là.

**La Propagation Temporelle (La Mémoire)**

Dans l'AR(1), l'erreur $\varepsilon_i$ ne disparaît pas immédiatement, car la valeur $Y_i$ qu'elle vient de modifier devient la **variable explicative** pour la période suivante.

Ainsi, l'erreur $\varepsilon_i$ influence non seulement $Y_i$, mais aussi toutes les valeurs futures $Y_{i+1}, Y_{i+2}, ...$ à travers la relation autorégressive.

1.   **L'effet sur** $Y_i$**:** Si $\varepsilon_i$ est grand (un "choc" important), il fait monter ou descendre la valeur $Y_i$.

2.   **L'influence sur** $Y_{i+1}$ **(via** $b$**):** La valeur future $Y_{i+1}$ dépend de $Y_i$ via le coefficient $b$ ($Y_{i+1}=a+b_i+\varepsilon_{i+1}$). 
    Comme $Y_i$ a été modifié par $\varepsilon_i$, l'erreur initiale $\varepsilon_i$ se répercute sur $Y_{i+1}$ à travers $Y_i$ et l'influence est mesurée par $b$.

3.   **L'influence sur** $Y_{i+2}$ **(via** $b^2$**):** De même, $Y_{i+2}$ dépend de $Y_{i+1}$, qui lui-même contenait déjà l'influence du choc $\varepsilon_i$. L'effet de l'erreur initiale $\varepsilon_i$ est donc transmis à $Y_{i+2}$ via $b^2$.

4.   **L'influence sur** $Y_{i+k}$**:** L'influence de l'erreur $\varepsilon_i$ sur une valeur lointaine $Y_{i+k}$ est mesurée par $b^k$.
En résumé, l'erreur $\varepsilon_i$ se propage dans le futur, mais son impact **décroît exponentiellement** avec la distance temporelle (puisque k augmente). C'est ce que l'on appelle la **corrélation temporelle**.

**L'Équilibre de la Variance Totale**

La **variance** mesure la dispersion de la variable. 

Puisque l'erreur $\varepsilon_i$ ne s'arrête jamais d'influencer les valeurs futures tant que le coefficient $b$ n'est pas nul, la variabilité totale observée dans la série $Y_i$ est la somme de toutes les influences passées. (si $b$ devient nul, l'effet de l'erreur s'arrête immédiatement car $Y_i$ ne dépend plus de $Y_{i-1}$ mais dépend uniquement de l'erreur actuelle $\varepsilon_i$ : la nouvelle équation sera $Y_i = a + (0 \times Y_{i-1}) + \varepsilon_i$)

La **variance totale** de la série $Y_i$ est le résultat d'un équilibre entre deux facteurs : la variance du bruit $\sigma^2$ et la mémoire du système (le coefficient $b$).

1.   **La variabilité injectée à chaque instant** : C'est la variance du bruit $\sigma^2$ qui mesure l'amplitude des fluctuations imprévisibles de $\varepsilon_i$. Si $\sigma^2$ est grande, cela signifie qu'il y a une grande quantité de variabilité nouvelle et imprévisible introduite à chaque pas de temps : chaque instant reçoit un "choc" important.

2.   **La quantité de mémoire du système = la propagation de cette variabilité** Ceci est donné par le coefficient $b$. Plus $b$ est grand, plus les erreurs passées continuent à influencer fortement les valeurs futures parce que 

Si la série est **stationnaire** (c'est-à-dire que $|b| < 1$), la variance totale est stable et finie. Elle est calculée par la formule : $Var(Y_i) = \frac{\sigma^2}{1 - b^2}$
Cette formule montre clairement que la variance totale du processus est **plus grande que la variance du bruit seul** (σ2), car l'effet de la propagation des erreurs passées s'accumule (via le dénominateur $1−b^2$).

Imaginons l'AR(1) comme un verre d'eau posé sur une table : le bruit ($\varepsilon_i$) est une petite goutte qui tombe dans l'eau à chaque instant (i). Dans une régression classique, l'ondulation de la goutte s'arrêterait immédiatement. Mais dans l'AR(1), si l'ondulation d'une goutte (l'erreur $\varepsilon_i$) est assez forte (si $b$ est grand), elle crée un mouvement qui influence l'emplacement des gouttes suivantes ($Y_{i+1}$, $Y_{i+2}$), ce qui fait que le niveau total d'agitation de l'eau (la variance totale) est bien supérieur à la simple force d'une seule goutte ($\sigma^2$).
:::


#### Propagation de la variance dans un AR(1)
C’est ici que l’AR(1) devient différent d’une régression classique.

Dans une régression ordinaire, l’erreur affecte uniquement la valeur courante.

Dans un AR(1), l’erreur affecte toute la suite, car :

$Y_{i+1} = a + bY_i + \varepsilon_{i+1}$

Donc si $\varepsilon_i$ est grand :

-   il fait monter ou descendre $Y_i$,

-   ce $Y_i$ influence $Y_{i+1}$ via le coefficient $b$,

-   et influence $Y_{i+2}$ via $b^2$, etc.

Ainsi, les erreurs se propagent dans le futur, et la variance totale est un équilibre entre :

-   la variabilité injectée à chaque instant ($\sigma^2$),

-   la quantité de mémoire du système (le coefficient $b$).

#### Variance stationnaire : définition explicite
Une variance stationnaire signifie :

1.  La variance de $Y_i$ reste la même pour tous les instants : elle ne diverge pas, ne monte pas, ne descend pas.

2.  Le processus garde une variabilité “stable” dans le temps.

Autrement dit : la série ne devient ni explosive, ni de plus en plus variable, ni de moins en moins variable.

Cette situation n’est possible que si $|b| < 1$.

-   Si $|b| < 1$, les erreurs se propagent mais finissent par s’atténuer → variance stable.

-   Si $|b| = 1$, les erreurs se cumulent sans limite → variance infinie (marche aléatoire).

-   Si $|b| > 1$, la série explose → variance diverge.

Dans le cas stationnaire ($|b| < 1$) :

$\text{Var}(Y_i) = \frac{\sigma^2}{1 - b^2}$

Cette formule signifie :

-   la variance totale est plus grande que la variance du bruit,

-   plus $b$ est grand, plus la variance de $Y_i$ augmente,

-   car plus $b$ est grand, plus les erreurs anciennes continuent à influencer les valeurs futures.

#### Corrélation temporelle : définition explicite
La corrélation mesure l’intensité du lien linéaire entre deux valeurs.

Dans un AR(1) stationnaire:

$\text{Corr}(Y_i, Y_{i-k}) = b^k$

Donc :

-   $\text{Corr}(Y_i, Y_{i-1}) = b$

-   $\text{Corr}(Y_i, Y_{i-2}) = b^2$

-   $\text{Corr}(Y_i, Y_{i-3}) = b^3$

-   etc.

C’est une corrélation **qui décroît exponentiellement avec la distance temporelle.**

#### Exemple chiffré complet
Prenons un AR(1) :

$Y_i = 0.8Y_{i-1} + \varepsilon_i, \quad \text{Var}(\varepsilon_i)=1$

**Variance stationnaire**

$\text{Var}(Y_i) = \frac{1}{1 - 0.8^2} = \frac{1}{1 - 0.64} = \frac{1}{0.36} \approx 2.78$

Donc :

-   la variance du processus est presque 3 fois la variance du bruit,

-   car b=0.8 propage beaucoup les erreurs passées.

### Simulation courte
Supposons Y_0 = 10.

-   $\varepsilon_1 = 0.5$

$Y_1 = 0.8 \times 10 + 0.5 = 8.5$

```         
-   $\varepsilon_2 = -0.2$
```

\$Y_2 = 0.8 \times 8.5 - 0.2 = 6.6

-   $\varepsilon_3 = 0.1$

$Y_3 = 0.8 \times 6.6 + 0.1 = 5.38$

La série décroît progressivement vers 0 (car $|b|<1$) et reste influencée par chaque erreur qui se propage.

### Corrélation temporelle
$\text{Corr}(Y_3, Y_2) = 0.8$

$\text{Corr}(Y_3, Y_1) = 0.8^2 = 0.64$

$\text{Corr}(Y_3, Y_0) = 0.8^3 = 0.512$

Résumé

Un AR(1) est un modèle où :

-   le présent est expliqué par le passé (régression),

-   le coefficient b crée une corrélation décroissante avec le passé,

-   la variance totale résulte de l’équilibre entre :

    -   la variance du bruit,

    -   la propagation de cette variance via b,

-   la variance stationnaire est une variance stable, finie, identique à travers le temps,

-   la constante a disparaît si on travaille sur une série centrée.

------------------------------------------------------------------------

## Modèle MA = Moving Average
Un modèle MA(q) exprime la valeur actuelle comme une **MOYENNE MOBILE** des **ERREURS PASSÉES**.

Les **BRUITS** (ou erreurs) sont liés les uns avec les autres.

Modèle MA(1) :

$Y_{j} = \varepsilon_{i} + \varepsilon_{i-1}$

$Y_{j-1} = \varepsilon_{i-1} + \varepsilon_{i-2}$

La corrélation entre $Y_j$ et $Y_{j-1}$ est donc due à la présence commune de $\varepsilon_{i-1}$ dans les deux équations. Donc corrélation ≠ 0 car il y a un terme commun.

Alors que si on fait la corrélation entre $Y_j$ et $Y_{j-2}$ :

$Y_{j} = \varepsilon_{i} + \varepsilon_{i-1}$

$Y_{j-2} = \varepsilon_{i-2} + \varepsilon_{i-3}$

**Il n'y a pas de terme commun entre les deux équations, donc corrélation = 0.**

Donc avec :

-   Un modèle AR(1) : la corrélation diminue exponentiellement avec le temps = autocorrélogramme.

-   Un modèle MA(1) : la corrélation est nulle au-delà du lag 1.

Donc à partir de la réprésentation de la corrélation en fonction du lag (autocorrélogramme), on peut deviner si le modèle est AR ou MA.

## Autocorrélogramme partiel
Un autocorrélogramme partiel permet de voir la corrélation entre

-   $Y_i$ et $Y_{i-p}$

-   ajusté sur $Y_{i-1}, Y_{i-2}, ..., Y_{i-(p-1)}$.

Corrélation spécifique ajustée entre le passé ± lointain et le présent, en ajustant sur tout ce qui se passe entre les deux.

Si on fait avec $p=2$\$ :

-   On regarde la corrélation entre $Y_i$ et $Y_{i-2}$

On a $Y_1$, $Y_2$, $Y_3$ (car p=2 donc on a besoin de 3 observations pour avoir un lag de 2 entre $Y_3$ et $Y_1$)

-   Processus autorégressif d'ordre 1: $Y_1$ agit sur $Y_2$ (lag 1) et $Y_2$ agit sur $Y_3$ (lag 1)

-   Corrélation entre $Y_1$ et $Y_3$ vaut la corrélation entre chaque $Y$ exponentiée par le lag : $b^2$

-   Mais si on ajuste sur $Y_2$, on enlève l'effet de $Y_2$ sur $Y_3$, donc $Y_1$ n'a plus d'effet sur $Y_3$ !

-   La corrélation entre $Y_1$ et $Y_3$ ajustée sur $Y_2$ vaut donc 0.

Donc l'autocorrélogramme partiel va donner 0 au-delà du lag 1 pour un modèle MA(1).

### Exemple 1
$Yi = bY{i-1} + cY{i-2} + \varepsilon_{i} + \varepsilon_{i-1}$

Modèle ARMA(2,1)

Dans un modèle ARMA(p, q),

-   p = ordre autorégressif (AR) → nombre de termes $Y$ retardés,

-   q = ordre moyenne mobile (MA) → nombre de termes $\varepsilon$ retardés.

1.  Partie AR (autorégressive)

    -   $Y_{i-1}$

    -   $Y_{i-2}$

Donc 2 termes AR → p = 2.

2.  Partie MA (moyenne mobile)

    -   $\varepsilon_i$ → toujours présent, ce n’est pas compté dans q

    -   $\varepsilon_{i-1}$

Le terme $\varepsilon_{i}$ est l’erreur courante : on ne le compte pas dans l’ordre.

On compte seulement les retards.

Donc il y a 1 terme MA retardé → q = 1.

### Exemple 2
Mesure de la TA tous les matins

$Y_i$ = TA du jour i : il y en a donc 365

#### Modèle autorégressif AR(1)
Les $Y_i$ ont une variance = 1 c'est à dire que la TA varie peu d'un jour à l'autre.

$\rightarrow$ $Y_{i} = bY_{i-1} + \varepsilon_i$

-   On peut pas faire une régression linéaire habituelle car $Y_{i}$ est à la fois à gauche et à droite de l'équation

-   si variance = 1 / variance stationnaire :

    -   $b$ = corrélation entre $Y_i$ et $Y_{i-1}$ quelque soit i

    -   avec le temps : corrélation entre $Y_i$ et $Y_{i-2}$ = $b^2$

    -   corrélation entre $Y_i$ et $Y_{i-k}$ = $b^k$

    -   avec le temps, la corrélation tend vers 0 de façon exponentielle

#### Modèle moyenne mobile MA(1)
$Y_i = \varepsilon_i + \varepsilon_{i-1}$

-   2 bruits : régression linéaire impossible

-   Le temps $i$ et le temps $i-1$ n'ont rien à voir l'un avec l'autre et sont liés au bruit

    -   Appareil de mesure a aussi un bruit et a un défaut ponctuel à un moment donné

    -   

        -   stress du jour qui influe sur le bruit du jour aussi

Globalement : dans la mesure du jour, le bruit observé vient d'un bruit issu de la mesure du jour + du bruit d'hier qui vient d'un contexte, influençant l'erreur de mesure qu'il y a dans $Y_i$ aujourd'hui.

-   Corrélation entre $Y_i$ et $Y_{i-1}$ = $\varrho$ et est due au fait qu'il y a un terme commun dans les 2 équations : $\varepsilon_{i-1}$

-   Corrélation entre $Y_i$ et $Y_{i-2}$ = 0 car pas de terme commun dans l'équation

#### Autocorrélogramme
Permet assez facilement de visualiser si le modèle est AR ou MA

#### Autocorrélogramme partiel
Corrélation entre $Y_i$ et $Y_{i-k}$ ajustée sur $Y_{i-1}, Y_{i-2}, ..., Y_{i-(k-1)}$ (ajusté sur toutes les mesures qu'on a entre les 2)

Si modèle AR(1) :

-   $Y_1$ agit sur $Y_2$ et $Y_2$ agit sur $Y_3$

-   Mais $Y_1$ agit sur $Y_3$ uniquement via $Y_2$

-   Donc si on ajuste sur $Y_2$, la corrélation entre $Y_1$ et $Y_3$ ajustée sur $Y_2$ = 0

**Donc modèle AR(1) dans autocorrélogramme partiel ressemble au modèle MA(1) dans autocorrélogramme classique.**

#### Processus stationnaire
= Processus dont les propriétés statistiques (moyenne, variance, autocorrélation) ne changent pas au cours du temps.

## Différents modèles : AR, MA, ARMA, SARMA, SARIMA, SARIMAX
On part d'un processus stationnaire.

1.  **Description** : évolution au cours du temps =\> MOYENNE STABLE ? VARIANCE STABLE ?

2.  **Modélisation** : autocorrélogramme et autoccorrélogramme partiel =\> modèle AR, MA ou ARMA ?

    -   voire SARMA : saisonnier (avec saisonnalité)

    -   voire SARIMA : avec dérive au cours du temps (écart à la stationnarité : I = Integrated c'est à dire qu'on intègre une tendance linéaire au modèle)

    -   voire SARIMAX : avec des variables explicatives (exogènes)

        -   exogènes = variables explicatives externes au modèle de la série chronologique

        -   $Y_i = bY_{i-1} + cY_{i-2} + \varepsilon_i + \varepsilon_{i-1} + d\vartheta$

------------------------------------------------------------------------

## Exemple d'analyse de données avec R
### Méthode graphique
Le fichier `gs` contient des données sur les tendances de recherche Google (sur recherche = suicide entre Juin 2017 et Juin 2022).

Voir si indice X = survenue du confinement modifie la recherche du mot "suicide"

Modèle = ARMAX (avec X = facteur exogène = confinement)

-   avec X

1.  Transformation de la variable `gs$suicide` en une série chronologique grâce à la fonction `ts` = timeserie

    -   si pas de fonction `ts`: on peut pas faire d'analyses sur séries chronologiques avec R.

    -   syntaxe de la fonction `ts` :

        -   `ts(data, frequency, start)`

        -   `data` = données,

        -   `frequency` = fréquence (ex: 52 semaines dans une année),

            -   permet de dire que la semaine $i$ et la semaine $i+52$ sont comparables (même semaine de l'année)

        -   `start` = date de début (ex: c(2017,24))

2.  Affichage de la série chronologique avec la fonction `plot`

```{r}
#construction du datasat pour série chronologique
gsts <- ts(data=gs$suicide,frequency=52,start=c(2017,24))
#affichage du graphique qui montre l'évolution de la série chronologique
plot(
    gsts,
    main = "Évolution temporelle du nombre standardisé de requêtes \nsur le moteur de recherche Google autour du mot « suicide »"
)
```

-   On considère "à la louche" que la variance est stationnaire au cours du temps.

-   Il y a peut-être une tendance à la baisse (le processus ne serait pas strictement stationnaire)

-   2 pics d'incidence sont observés, respectivement vers la fin des années 2018 et 2021 : cela peut faire craindre des problèmes d’écart à l’hypothèse de normalité dans les analyses à venir.

Pour obtenir un processus stationnaire : une solution classique est de modéliser la différence : $\Delta Y_t = Y_t - Y_{t-1}$

```{r}
plot.ts(diff(gsts), main = "Évolution temporelle de la différence Delta Y_t = Y_t - Y_{t-1}. \nLe processus est rendu stationnaire, mais les écarts à la normalité persistent.")
```

-   Le processus est beaucoup plus stationnaire

-   Il persiste un pic en 2019, qui fait que la variance est pas complètement constante, et la normalité des résidues $\varepsilon{i}$ ne sera pas parfaite.

3.  Autocorrélogramme

Fonction `acf` (auto correlation function) permet de tracer l'autocorrélogramme

Faite sur la différence de la série chronologique pour rendre le processus stationnaire

Syntaxe :

-   `acf(x, lag.max, main)`

    -   `x` = série chronologique

    -   `lag.max` = nombre maximum de lags à afficher

    -   `main` = titre du graphique

```{r}
acf(diff(gsts), lag.max= 60, main="Autocorrélogramme de la différence de la série chronologique")
```

-   1ère mesure est fortement corrélée avec la mesure précédente (lag 1) (corrélation entre $Y_i$ et $Y_i$ donc on s'en fiche un peu)

-   2e mesure : corrélation négative $Y_i$ et $Y_{i-1}$ largement au dessus du seuil de significativité (pointillés bleus)

-   3e mesure : aucune corrélation

Il y a un décrochage assez net pour k \> 1 (lag \> 1), ce qui est en faveur d'un modèle MA(1).

**Donc au moins une compensante MA(1) !**

4.  Autocorrélogramme partiel

Fonction `pacf` (partial auto correlation function) permet de tracer l'autocorrélogramme partiel

Syntaxe :

-   `pacf(x, lag.max, main)`

    -   `x` = série chronologique

    -   `lag.max` = nombre maximum de lags à afficher

    -   `main` = titre du graphique

```{r}
pacf(diff(gsts), lag.max= 60, main="Autocorrélogramme partiel de la différence de la série chronologique")
```

-   1ère corrélation partielle est élevée (lag 1)

-   2e corrélation partielle est statistiquement significative (lag 2)

-   Mais chute énorme de corrélation entre 1ère, 2e...

Donc probablement une composante AR(1)

Au total : probablement un modèle ARMA(1,1)

### Méthode automatique avec fonction `auto.arima`
1.  Sans variable exogène

Fonction `auto.arima` du package `forecast` permet de faire une sélection automatique du modèle ARIMA le plus adapté aux données.

Pour Falissard : c'est un piège car automatique et n'oblige pas à réfléchir !

Syntaxe :

-   `auto.arima(y, xreg, seasonal, trace)`

    -   `y` = série chronologique

    -   `xreg` = matrice de variables explicatives exogènes (optionnel)

    -   `seasonal` = si TRUE, recherche de composantes saisonnières (optionnel)

    -   `trace` = si TRUE, affiche les modèles testés (optionnel)

```{r}
auto.arima(diff(gsts), trace=TRUE)
```

------------------------------------------------------------------------

**Ici : Modèle ARIMA(1,0,1) est sélectionné donc équivalent à ARMA(1,1)**

-   Avec I pour Integrated = 0 (= dérive au cours du temps) (car on a différencié la série chronologique initiale)

-   (1,0,1) car :

    -   p = 1 (AR(1))

    -   d = 0 (Integrated)

        -   d=1 : ça veut dire qu'il y a une tendance linéaire à la baisse, ce qui aurait été le cas si on avait pris \`gsts\`\`

        -   d=2 : tendance quadratique, c'est à dire courbe en U ou en cloche

    -   q = 1 (MA(1))

-   sans composante saisonnière (car pas de saisonnalité dans les données) (s'il y en avait une, ça aurait été noté ARIMA(p,d,q)(P,D,Q)\[s\] avec s = saisonnalité)

2.  Avec variable exogène

On regarde s'il y a une rupture le 17/03/20 = confinement

```         
1.  Création de la variable exogène
```

Variable `x` construite :

-   Elle vaut –1 pour les semaines avant la semaine 145 (date du 17 Mars 2020)

-   Et vaut 0 jusqu’en Juin 2022.

-   Utiliser la fonction `rep` pour répéter les valeurs *(c'est à dire que `rep`(-1,145)`crée un vecteur de 145 valeurs égales à -1 et`rep(0,115)\` crée un vecteur de 115 valeurs égales à 0)*

```{r}
x <- c(rep(-1,145), rep(0,115))
```

Puis même modèle `res` mais en y intégrant la variable exogène `x` via l'argument `xreg`

Fonction `arima` permet de faire une modélisation ARIMA avec des variables exogènes

Syntaxe :

-   `arima(x, order, xreg)`

    -   `x` = série chronologique

    -   `order` = ordre du modèle ARIMA (p,d,q) (ici, on sait que c'est (1,0,1) d'après l'analyse précédente)

    -   `xreg` = matrice de variables explicatives exogènes

```{r}
res <- arima(diff(gsts), order = c(1,0,1), xreg = x)
res
```

------------------------------------------------------------------------

Estimation de l'effet de `x` avec son écart-type.

Si on fait le rapport de `x` sur son écart-type, on obtient une statistique de test de Wald

On le compare à une loi normale centrée réduite pour obtenir une p-value : c'est à dire qu'on le compare à 1,96 (seuil de significativité à 5%).

Le paramètre de `x`= 0.0855

L'écart-type de `x` = 0.0472

Rapport : 0.0855 / 0.0472 = 1.81 \< 1.96 donc p-value \> 0.05

**Donc pas d'effet significatif du confinement sur les recherches Google autour du mot "suicide".**

------------------------------------------------------------------------

## Fonction `stl()`
Fonction graphique purement exploratoire qui permet de décomposer une série chronologique en 3 composantes :

-   données originales

-   saisonnalité

-   tendance

-   résidus

Syntaxe :

-   `stl(x, s.window, t.window)`

    -   `x` = série chronologique

    -   `s.window` = fenêtre de lissage pour la saisonnalité (ex: "periodic")

    -   `t.window` = fenêtre de lissage pour la tendance (ex: 13)

```{r}
plot(
    stl(
        gsts,
        s.window="periodic"), 
        main="Analyse graphique exploratoire d’une série chronologique à l’aide de la fonction `stl()`proposant une \ndécomposition en trois termes : une composante périodique, une composante de tendance et un résidu."
    )
```

-   On observe une saisonnalité annuelle avec un pic fin Août (avant la rentrée scolaire)
:::

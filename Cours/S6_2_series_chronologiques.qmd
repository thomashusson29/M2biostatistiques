---
title: "S6_1_Donnees_manquantes"
format:
    html:
        toc: true
        toc-depth: 5
        toc-title: "Table of contents"
        toc-location: left
        toc-sticky: true
        number-sections: true
        theme: default

    docx:
        toc: true
        toc-depth: 5

    pdf:
        toc: true
        toc-depth: 5
        pdf-engine: xelatex
        number-sections: true
        header-includes: |
            % \usepackage{fontspec} 
            % \setmainfont{Ubuntu}
            \usepackage{etoolbox}
            \renewcommand{\contentsname}{}
            \AtBeginDocument{
                \addtocontents{toc}{\protect\smallskip}
                \let\oldtableofcontents\tableofcontents
                \renewcommand{\tableofcontents}{
                \begingroup
                    \footnotesize
                    \setlength{\parskip}{2pt}
                    \oldtableofcontents
                \endgroup
                }
            }
            \setcounter{tocdepth}{5}
            \makeatletter
            \renewcommand{\@tocrmarg}{0pt}
            \makeatother
            \usepackage{fvextra}
            \usepackage[section]{placeins}
            \usepackage{needspace}
            \usepackage{float}
            \floatplacement{figure}{H}
            \floatplacement{table}{H}
            \newcommand{\sectionbreak}{\needspace{5\baselineskip}}
            \setlength{\parindent}{0pt}
            \setlength{\parskip}{4pt}
            \usepackage[most]{tcolorbox}
            \usepackage{color}
            \definecolor{lightgray}{gray}{0.95}
            \newtcolorbox{graybox}{colback=gray!10!white,colframe=black,boxrule=0.6pt,arc=1mm,left=6pt,right=6pt,top=4pt,bottom=4pt}
            \newtcolorbox{codebox}{breakable,colback=blue!5!white,colframe=blue!50!black,boxrule=0.5pt,arc=1mm,left=4pt,right=4pt,top=3pt,bottom=3pt}
            \DefineVerbatimEnvironment{CodeBoxContent}{Verbatim}{fontsize=\small,breaklines,breakanywhere}
            \renewcommand{\thesection}{\arabic{section}}
            \renewcommand{\thesubsection}{\thesection.\Alph{subsection}}
            \renewcommand{\thesubsubsection}{\thesubsection.\arabic{subsubsection}}

geometry: margin=2.5cm
---

```{r}
#| label: setup
#| include: false
#| echo: false

library(plotrix)
library(viridisLite)
library(ggplot2)
library(survminer)
library(treemap)
library(psy)
library(qgraph)
library(ape)
library(survival)
library(httpgd)
library(psy)
knitr::opts_chunk$set(echo = TRUE)

load("~/Documents/Projets/M2biostatistiques/Cours/CUSM_data/CUSM")
data(expsy)
alzh = read.csv("~/Documents/Projets/M2biostatistiques/Cours/alzheimer.csv")
```

# Introduction aux séries chronologiques

Méthodes SARIMAX / ARMA / ARIMA

-   SARIMAX : Seasonal Autoregressive Integrated Moving Average with eXogenous regressors

-   ARMA : Autoregressive Moving Average

-   ARIMA : Autoregressive Integrated Moving Average

## Modèle auto-régressif d'ordre 1 :

Soit une variable $Y$ mesurée au temps $i$

-   Régression de Y sur ce qui se passe au temps $i-1$ : $Y_(i) = (a) + bY_(i-1) + \epsilon$

    -   Ce qui se passe au temps $i$ = une fonction linéaire de ce qui se passe au temps $i-1$ + un bruit aléatoire

    -   Processus auto-régressif d'ordre 1 (AR(1))

On observe une variable Y dans le temps : $Y_1$, $Y_2$, $Y_3$.

Le modèle AR(1) écrit :

$Y_i = a + bY_{i-1} + \varepsilon_i$

où $\varepsilon_i$ est un bruit aléatoire indépendant d’un instant à l’autre, c'est à dire qu'il n'y a pas de corrélation entre les $\varepsilon_i$ entre eux

### Qu’est-ce qu’une régression (dans ce contexte) ?

Une régression est un modèle qui cherche à expliquer une variable par une autre, en supposant une relation linéaire.

Dans un AR(1), la variable explicative n’est pas une autre variable, mais l**a même variable observée juste avant**, c’est-à-dire $Y_{i-1}$.

C’est donc une régression temporelle, ou auto-régression :

-   $Y_{i-1}$ est l’information disponible au temps précédent,

-   $Y_i$ est ce qu’on cherche à prédire,

-   $\varepsilon_i$ représente la part de $Y_i$ qui n’est pas expliquée par $Y_{i-1}$.

Autrement dit : le présent dépend linéairement du passé (*= c'est une fonction linéaire*), plus un terme aléatoire.

### Pourquoi dit-on souvent que « $a = 0$ » ?

Le coefficient $a$ est une constante, comme l’intercept dans une régression classique.

Cependant, dans la majorité des cours et logiciels :

1.  La série est centrée avant modélisation.

    -   Centrer consiste à soustraire la moyenne : $Y_i \leftarrow Y_i - \bar Y$.

    -   Une série centrée a une moyenne égale à 0.

2.  Si la série est centrée, alors le terme constant devient inutile.

    -   En effet, **la constante sert justement à ajuster une moyenne non nulle**.

    -   Donc, dans une série centrée : $Y_i = bY_{i-1} + \varepsilon_i$

    -   et on peut prendre $a = 0$ par convention.

### Variance : définition générale

La variance est une mesure de la dispersion d’une variable autour de sa moyenne.

-   Une grande variance = la variable varie beaucoup.

-   Une petite variance = la variable varie peu.

Mathématiquement :

$\text{Var}(Y) = E[(Y - E[Y])^2]$

Dans un AR(1), il y a deux sources de variance :

1.  la variance du bruit $\varepsilon_i$

2.  la propagation de ce bruit à travers le coefficient $b$

### Variance du bruit : ce que c’est exactement

Le bruit $\varepsilon_i$ est un terme aléatoire supposé :

-   d’espérance 0 (= moyenne, c'està dire que le bruit n'a pas de tendance moyenne à être positif ou négatif),

-   indépendant d’un instant à l’autre,

-   de variance constante $\sigma^2$.

Cette variance $\sigma^2$ mesure l’amplitude des fluctuations imprévisibles.

C’est la “source” de variabilité du modèle.

Si $\sigma^2$ est grande : chaque instant reçoit un choc important.

Si $\sigma^2$ est petite : les variations imprévisibles sont faibles.

### Propagation de la variance dans un AR(1)

C’est ici que l’AR(1) devient différent d’une régression classique.

Dans une régression ordinaire, l’erreur affecte uniquement la valeur courante.

Dans un AR(1), l’erreur affecte toute la suite, car :

$Y_{i+1} = a + bY_i + \varepsilon_{i+1}$

Donc si $\varepsilon_i$ est grand :

-   il fait monter ou descendre $Y_i$,

-   ce $Y_i$ influence $Y_{i+1}$ via le coefficient $b$,

-   et influence $Y_{i+2}$ via $b^2$, etc.

Ainsi, les erreurs se propagent dans le futur, et la variance totale est un équilibre entre :

-   la variabilité injectée à chaque instant ($\sigma^2$),

-   la quantité de mémoire du système (le coefficient $b$).

### Variance stationnaire : définition explicite

Une variance stationnaire signifie :

1.  La variance de $Y_i$ reste la même pour tous les instants : elle ne diverge pas, ne monte pas, ne descend pas.

2.  Le processus garde une variabilité “stable” dans le temps.

Autrement dit : la série ne devient ni explosive, ni de plus en plus variable, ni de moins en moins variable.

Cette situation n’est possible que si $|b| < 1$.

-   Si $|b| < 1$, les erreurs se propagent mais finissent par s’atténuer → variance stable.

-   Si $|b| = 1$, les erreurs se cumulent sans limite → variance infinie (marche aléatoire).

-   Si $|b| > 1$, la série explose → variance diverge.

Dans le cas stationnaire ($|b| < 1$) :

$\text{Var}(Y_i) = \frac{\sigma^2}{1 - b^2}$

Cette formule signifie :

-   la variance totale est plus grande que la variance du bruit,

-   plus $b$ est grand, plus la variance de $Y_i$ augmente,

-   car plus $b$ est grand, plus les erreurs anciennes continuent à influencer les valeurs futures.

### Corrélation temporelle : définition explicite

La corrélation mesure l’intensité du lien linéaire entre deux valeurs.

Dans un AR(1) stationnaire:

$\text{Corr}(Y_i, Y_{i-k}) = b^k$

Donc :

-   $\text{Corr}(Y_i, Y_{i-1}) = b$

-   $\text{Corr}(Y_i, Y_{i-2}) = b^2$

-   $\text{Corr}(Y_i, Y_{i-3}) = b^3$

-   etc.

C’est une corrélation **qui décroît exponentiellement avec la distance temporelle.**

### Exemple chiffré complet

Prenons un AR(1) :

$Y_i = 0.8Y_{i-1} + \varepsilon_i, \quad \text{Var}(\varepsilon_i)=1$

**Variance stationnaire**

$\text{Var}(Y_i) = \frac{1}{1 - 0.8^2} = \frac{1}{1 - 0.64} = \frac{1}{0.36} \approx 2.78$

Donc :

-   la variance du processus est presque 3 fois la variance du bruit,

-   car b=0.8 propage beaucoup les erreurs passées.

## Simulation courte

Supposons Y_0 = 10.

-   $\varepsilon_1 = 0.5$

$Y_1 = 0.8 \times 10 + 0.5 = 8.5$

```         
-   $\varepsilon_2 = -0.2$
```

\$Y_2 = 0.8 \times 8.5 - 0.2 = 6.6

-   $\varepsilon_3 = 0.1$

$Y_3 = 0.8 \times 6.6 + 0.1 = 5.38$

La série décroît progressivement vers 0 (car $|b|<1$) et reste influencée par chaque erreur qui se propage.

## Corrélation temporelle

$\text{Corr}(Y_3, Y_2) = 0.8$

$\text{Corr}(Y_3, Y_1) = 0.8^2 = 0.64$

$\text{Corr}(Y_3, Y_0) = 0.8^3 = 0.512$

Résumé

Un AR(1) est un modèle où :

-   le présent est expliqué par le passé (régression),

-   le coefficient b crée une corrélation décroissante avec le passé,

-   la variance totale résulte de l’équilibre entre :

    -   la variance du bruit,

    -   la propagation de cette variance via b,

-   la variance stationnaire est une variance stable, finie, identique à travers le temps,

-   la constante a disparaît si on travaille sur une série centrée.

------------------------------------------------------------------------

# Modèle MA = Moving Average

Un modèle MA(q) exprime la valeur actuelle comme une **MOYENNE MOBILE** des **ERREURS PASSÉES**.

Les **BRUITS** (ou erreurs) sont liés les uns avec les autres.

Modèle MA(1) :

$Y_{j} = \epsilon_{i} + \epsilon_{i-1}$

$Y_{j-1} = \epsilon_{i-1} + \epsilon_{i-2}$

La corrélation entre $Y_j$ et $Y_{j-1}$ est donc due à la présence commune de $\epsilon_{i-1}$ dans les deux équations.

Donc corrélation ≠ 0 car il y a un terme commun.

Alors que si on fait la corrélation entre $Y_j$ et $Y_{j-2}$ :

$Y_{j} = \epsilon_{i} + \epsilon_{i-1}$

$Y_{j-2} = \epsilon_{i-2} + \epsilon_{i-3}$

**Il n'y a pas de terme commun entre les deux équations, donc corrélation = 0.**

Donc avec :

-   Un modèle AR(1) : la corrélation diminue exponentiellement avec le temps = autocorrélogramme.

-   Un modèle MA(1) : la corrélation est nulle au-delà du lag 1.

![](images/Untitled.png)

Donc à partir de la réprésentation de la corrélation en fonction du lag (autocorrélogramme), on peut deviner si le modèle est AR ou MA.


# Autocorrélogramme partiel

Un autocorrélogramme partiel permet de voir la corrélation entre 

-   $Y_i$ et $Y_{i-p}$ 

-   ajusté sur $Y_{i-1}, Y_{i-2}, ..., Y_{i-(p-1)}$.

Corrélation spécifique ajustée entre le passé ± lointain et le présent, en ajustant sur tout ce qui se passe entre les deux.

Si on fait avec $p=2$$ : 

-   On regarde la corrélation entre $Y_i$ et $Y_{i-2}$

On a $Y_1$, $Y_2$, $Y_3$ (car p=2 donc on a besoin de 3 observations pour avoir un lag de 2 entre $Y_3$ et $Y_1$)

-   Processus autorégressif d'ordre 1: $Y_1$ agit sur $Y_2$ (lag 1) et $Y_2$ agit sur $Y_3$ (lag 1)

-   Corrélation entre $Y_1$ et $Y_3$ vaut la corrélation entre chaque $Y$ exponentiée par le lag : $b^2$

-   Mais si on ajuste sur $Y_2$, on enlève l'effet de $Y_2$ sur $Y_3$, donc $Y_1$ n'a plus d'effet sur $Y_3$ ! 

-   La corrélation entre $Y_1$ et $Y_3$ ajustée sur $Y_2$ vaut donc 0.

Donc l'autocorrélogramme partiel va donner 0 au-delà du lag 1 pour un modèle MA(1).

## Exemple

$Yi = bY{i-1} + cY{i-2} + \epsilon_{i} + \epsilon_{i-1}$

Modèle ARMA(2,1)

Dans un modèle ARMA(p, q),

-   p = ordre autorégressif (AR) → nombre de termes $Y$ retardés,

-   q = ordre moyenne mobile (MA) → nombre de termes $\varepsilon$ retardés.

1. Partie AR (autorégressive)

    -   $Y_{i-1}$

    -   $Y_{i-2}$

Donc 2 termes AR → p = 2.

2. Partie MA (moyenne mobile)

    -   $\varepsilon_i$  → toujours présent, ce n’est pas compté dans q

    -   $\varepsilon_{i-1}$

Le terme $\varepsilon_{i}$ est l’erreur courante : on ne le compte pas dans l’ordre.

On compte seulement les retards.

Donc il y a 1 terme MA retardé → q = 1.